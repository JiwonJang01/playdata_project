{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afb621-c497-4eac-897d-eb1e3bde21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 한권 추출 ##\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def bookRV(num):\n",
    "    url = \"https://product.kyobobook.co.kr/api/review/list?page={}&pageLimit=10&reviewSort=001&revwPatrCode=000&saleCmdtid=S000202652825\"  #해당 도서 상세 페이지 url\n",
    "    response = requests.get(url.format(num), headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            review_list = data['data']['reviewList']\n",
    "\n",
    "            # 필요한 정보를 추출하여 리스트로 정리 후 추출\n",
    "            formatted_reviews = []\n",
    "            for review in review_list:\n",
    "                formatted_review = {\n",
    "                    'saleCmdtid': review['saleCmdtid'],\n",
    "                    'cmdtName': review['cmdtName'],\n",
    "                    'revwNum': review['revwNum'],\n",
    "                    'cretDttm': review['cretDttm'],\n",
    "                    'revwCntt': review['revwCntt'],\n",
    "                    'revwRvgr': review['revwRvgr'],\n",
    "                    'reviewRecommendCount': review['reviewRecommendCount']\n",
    "                }\n",
    "                formatted_reviews.append(formatted_review)\n",
    "\n",
    "            return formatted_reviews\n",
    "    else: # 실패시\n",
    "        print(f\"Failed to retrieve the page for page {num}.\")\n",
    "        return []\n",
    "\n",
    "book_rvlist = []  # 리뷰 쌓을 리스트\n",
    "for num in range(1, 11):   # 리뷰 페이지 순환\n",
    "    book_rvlist.extend(bookRV(num))\n",
    "\n",
    "# 데이터 프레임으로 전환\n",
    "df_review = pd.DataFrame(book_rvlist) \n",
    "df_review.to_csv(\"쇼펜하우어아포리즘:당신의인생이왜힘들지않아야한다고생각하십니까S000202652825.csv\", encoding='utf-8-sig', index=False)  # CSV저장\n",
    "df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922c9865-180d-49ee-b334-fa8346412176",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "clear\n",
      "all clear\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ssl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#soup = BeautifulSoup(html_content, 'html.parser')\n",
    "data_id_list = []\n",
    "\n",
    "# saleCmdtid가 들어있는 CSV 파일 읽고 데이터 프레임으로\n",
    "csv_file_path = r\"2023best.csv\"  # CSV 파일 경로\n",
    "data_id_df = pd.read_csv(csv_file_path)  # CSV 파일을 DataFrame으로 읽기\n",
    "\n",
    "# 'data_id' 열의 값을 리스트로 가져오기\n",
    "data_id_list = data_id_df['SCID'].tolist()\n",
    "\n",
    "def bookRV(num, data_id):\n",
    "    ssl_ctx = ssl.create_default_context()\n",
    "    ssl_ctx.set_ciphers('DEFAULT@SECLEVEL=1')\n",
    "\n",
    "    # num=리뷰페이지 data_id위에서 만든 리스트에서 가져올 것\n",
    "    url = f\"https://product.kyobobook.co.kr/api/review/list?page={num}&pageLimit=10&reviewSort=001&revwPatrCode=000&saleCmdtid={data_id}\"\n",
    "    response = requests.get(url, verify=True, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        review_list = data['data']['reviewList']\n",
    "\n",
    "        # 필요한 정보를 추출하여 리스트로 정리 후 추출\n",
    "        formatted_reviews = []\n",
    "        for review in review_list:\n",
    "            formatted_review = {\n",
    "                'saleCmdtid': review['saleCmdtid'],\n",
    "                'cmdtName': review['cmdtName'],\n",
    "                'revwNum': review['revwNum'],\n",
    "                'revwCntt': review['revwCntt'],\n",
    "                'revwRvgr': review['revwRvgr'],\n",
    "                'reviewRecommendCount': review['reviewRecommendCount']\n",
    "            }\n",
    "            formatted_reviews.append(formatted_review)\n",
    "        return formatted_reviews\n",
    "        \n",
    "    else:  # 실패시\n",
    "        print(f\"Failed to retrieve the page for page {num}.\")\n",
    "        return []\n",
    "\n",
    "import re\n",
    "\n",
    "for data_id in data_id_list:   # data_id순환\n",
    "    book_rvlist = []    # 리뷰 쌓을 리스트\n",
    "    cmdtName = None  # cmdtName 변수 초기화\n",
    "    for num in range(1, 10):   # 리뷰 페이지 순환\n",
    "        if not cmdtName:\n",
    "            cmdtName = bookRV(1, data_id)[0]['cmdtName']  # 첫 번째 페이지의 cmdtName 가져오기\n",
    "        book_rvlist.extend(bookRV(num, data_id))\n",
    "        \n",
    "    # 파일명에 있는 공백을 언더스코어로 대체하여 저장\n",
    "    cmdtName = re.sub(r'[^\\w\\s]', '', cmdtName).replace(\" \", \"_\")\n",
    "    df_review = pd.DataFrame(book_rvlist)\n",
    "    df_review.to_csv(f\"last2023\\\\{cmdtName}_{data_id}.csv\", encoding='utf-8-sig', index=False)\n",
    "    print(\"clear\")\n",
    "    \n",
    "# 작업 끝나면 clear출력\n",
    "print(\"all clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09e756a1-7404-4cfb-b3b8-c9be14a9274f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"2005-03-17 22:28:00\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 2460. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(output_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#utf8일 경우 encoding 필요 없음\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 함수 호출\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m saveUtf()\n",
      "Cell \u001b[1;32mIn[17], line 61\u001b[0m, in \u001b[0;36msaveUtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(original_file_path) \u001b[38;5;66;03m#utf8일 경우 encoding 필요 없음\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 'cretDttm' 컬럼을 날짜 형식으로 변환\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcretDttm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcretDttm\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 날짜 형식을 '%Y-%m-%d' 형태의 문자열로 변경\u001b[39;00m\n\u001b[0;32m     64\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcretDttm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcretDttm\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1050\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1050\u001b[0m         values \u001b[38;5;241m=\u001b[39m convert_listlike(arg\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1051\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:453\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m    455\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    456\u001b[0m     arg,\n\u001b[0;32m    457\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:484\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    474\u001b[0m     arg,\n\u001b[0;32m    475\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    480\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    481\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\tslibs\\strptime.pyx:530\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\tslibs\\strptime.pyx:351\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"2005-03-17 22:28:00\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 2460. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 원본 CSV 파일 경로\n",
    "original_file_path = 'ple'\n",
    "\n",
    "def saveAnsi():\n",
    "    output_file_path = 'K_Book_Reveiw_all_ANSI.csv'\n",
    "    # CSV 파일을 읽어오기 (latin1 인코딩 사용)\n",
    "    df = pd.read_csv(original_file_path, encoding='cp949')\n",
    "\n",
    "    # 'cretDttm' 컬럼에서 시간 데이터를 제거하고 날짜 부분만을 사용하여 날짜 형식으로 변환\n",
    "    df['cretDttm'] = pd.to_datetime(df['cretDttm'].str.split().str[0], format='%Y-%m-%d')\n",
    "\n",
    "    # 날짜 형식을 '%Y-%m-%d' 형태의 문자열로 변경\n",
    "    df['cretDttm'] = df['cretDttm'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    #  'revwRvgr' 컬럼에 2.5곱하기\n",
    "    df['revwRvgr'] = df['revwRvgr'] * 2.5\n",
    "\n",
    "    # 만약 'revwRvgr' 컬럼이 정수형이라면, 데이터 타입을 다시 정수형으로 변환\n",
    "    df['revwRvgr'] = df['revwRvgr'].astype(int)\n",
    "\n",
    "    # 변경된 CSV 파일 저장하기\n",
    "    df.to_csv(output_file_path, encoding='ANSI', index=False)\n",
    "\n",
    "# 함수 호출\n",
    "#saveAnsi()\n",
    "\n",
    "    \n",
    "def saveUtf():\n",
    "    # 변경된 CSV 파일 경로\n",
    "    output_file_path = 'K_Book_Reveiw_all_UTF8.csv'\n",
    "\n",
    "    # CSV 파일을 읽어오기\n",
    "    df = pd.read_csv(original_file_path) #utf8일 경우 encoding 필요 없음\n",
    "\n",
    "    # 'cretDttm' 컬럼을 날짜 형식으로 변환\n",
    "    df['cretDttm'] = pd.to_datetime(df['cretDttm'])\n",
    "\n",
    "    # 날짜 형식을 '%Y-%m-%d' 형태의 문자열로 변경\n",
    "    df['cretDttm'] = df['cretDttm'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    #  'revwRvgr' 컬럼에 2.5곱하기\n",
    "    df['revwRvgr'] = df['revwRvgr'] * 2.5\n",
    "\n",
    "    # 만약 'revwRvgr' 컬럼이 정수형이라면, 데이터 타입을 다시 정수형으로 변환\n",
    "    df['revwRvgr'] = df['revwRvgr'].astype(int)\n",
    "\n",
    "    # 변경된 CSV 파일 저장하기\n",
    "    df.to_csv(output_file_path, index=False) #utf8일 경우 encoding 필요 없음\n",
    "\n",
    "saveUtf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ac36f4-9143-47ce-94cc-5770b9bf6276",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 40: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# CSV 파일을 DataFrame으로 읽기\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 필요한 컬럼만 선택\u001b[39;00m\n\u001b[0;32m     42\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaleCmdtid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain_category\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:639\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2021\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc0 in position 40: invalid start byte"
     ]
    }
   ],
   "source": [
    "## 추출한 csv합치기 ##\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 합칠 CSV 파일들이 들어있는 폴더 경로\n",
    "folder_path = 'category'\n",
    "\n",
    "# # 모든 파일을 저장할 빈 DataFrame 생성\n",
    "# combined_df = pd.DataFrame()\n",
    "\n",
    "# # 폴더 내의 모든 파일에 대해 반복\n",
    "# for file_name in os.listdir(folder_path):\n",
    "#     if file_name.endswith('.csv'):  # 확장자가 .csv인 파일만 처리\n",
    "#         file_path = os.path.join(folder_path, file_name)\n",
    "#         # CSV 파일을 DataFrame으로 읽기\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         # 필요한 컬럼만 선택\n",
    "#         df = df[['saleCmdtid', 'cmdtName', 'revwNum', 'cretDttm', 'revwCntt', 'revwRvgr', 'reviewRecommendCount']]\n",
    "#         # DataFrame을 combined_df에 추가\n",
    "#         combined_df = combined_df.append(df, ignore_index=True)\n",
    "\n",
    "# # 모든 데이터를 담은 하나의 CSV 파일로 저장\n",
    "# combined_df.to_csv(\"k_review\\K_Book_Reveiw22.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# # 합칠 CSV 파일들이 들어있는 폴더 경로\n",
    "# folder_path = 'C:/Users/Playdata2/Downloads/bookReview2023/bookReview'\n",
    "# # 모든 파일을 저장할 빈 DataFrame 생성을 위한 리스트\n",
    "dfs = []\n",
    "# 폴더 내의 모든 파일에 대해 반복\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'):  # 확장자가 .csv인 파일만 처리\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # CSV 파일을 DataFrame으로 읽기\n",
    "        df = pd.read_csv(file_path)\n",
    "        # 필요한 컬럼만 선택\n",
    "        df = df[['saleCmdtid', 'main_category']]\n",
    "        # DataFrame을 리스트에 추가\n",
    "        dfs.append(df)\n",
    "# 리스트에 저장된 모든 DataFrame을 하나로 합치기\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# 모든 데이터를 담은 하나의 CSV 파일로 저장\n",
    "combined_df.to_csv(\"main_category_utf8.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c5a0e-a337-4b16-8baf-215cc68a0fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c02e5f-bdf0-4d20-a6ee-66fe4c7e8072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179655c3-884a-447b-9372-ecb480ca13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # CSV 파일 읽기 (인코딩을 명시적으로 지정)\n",
    "# df = pd.read_csv('k_review_finish_utf8.csv', encoding='utf-8')\n",
    "\n",
    "# # 각 셀에 대해 &로 시작하고 ;으로 끝나는 패턴을 삭제\n",
    "# df = df.applymap(lambda x: '' if isinstance(x, str) and x.startswith('&') and x.endswith(';') else x)\n",
    "\n",
    "# # 수정된 데이터프레임을 새로운 CSV 파일로 저장\n",
    "# df.to_csv(, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 열기 (비 ASCII 문자를 포함한 경우)\n",
    "with open('k_review_pl_finish_utf8.csv', encoding='utf-8') as f:\n",
    "    # CSV 파일을 DataFrame으로 읽기\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "# 각 셀에 대해 &로 시작하고 ;으로 끝나는 패턴을 삭제\n",
    "df = df.applymap(lambda x: '' if isinstance(x, str) and x.startswith('&') and x.endswith(';') else x)\n",
    "\n",
    "# 수정된 데이터프레임을 새로운 CSV 파일로 저장\n",
    "df.to_csv('k_review_final_utf8.csv', index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3414c0fb-1ee6-4f8e-a9e3-2fc86d0664cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일의 인코딩: None\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "# 파일 읽기\n",
    "with open('k_review_finish_utf8.csv', 'rb') as f:\n",
    "    # 파일 내용 읽기\n",
    "    data = f.read()\n",
    "\n",
    "# 파일의 인코딩 감지\n",
    "result = chardet.detect(data)\n",
    "print(\"파일의 인코딩:\", result['encoding'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4059b7-c23a-4f58-b2fa-3b3bc82c9298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb8 in position 72: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_review_finish_utf8.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb8 in position 72: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open('k_review_finish_utf8.csv', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open('output_file.csv', 'w', encoding='utf-8') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d70d4eb-8c73-44b4-a53b-1d7e63c9811f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>cmdtName</th>\n",
       "      <th>revwScore</th>\n",
       "      <th>reviewRecommendCount</th>\n",
       "      <th>revwCntt</th>\n",
       "      <th>revwNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>마법천자문 44: 죄를 씻어 내라! 목욕할 욕</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1316001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>대변동: 위기, 선택, 변화</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>04-04</td>\n",
       "      <td>909679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>팩트풀니스</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1234567890</td>\n",
       "      <td>11616342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>내게 무해한 사람</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11111111111</td>\n",
       "      <td>1545516.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.790000e+12</td>\n",
       "      <td>일론 머스크</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.11E+11</td>\n",
       "      <td>13229101.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ISBN                   cmdtName  revwScore  reviewRecommendCount  \\\n",
       "0  9.790000e+12  마법천자문 44: 죄를 씻어 내라! 목욕할 욕       10.0                   0.0   \n",
       "1  9.790000e+12            대변동: 위기, 선택, 변화       10.0                   0.0   \n",
       "2  9.790000e+12                      팩트풀니스       10.0                   0.0   \n",
       "3  9.790000e+12                  내게 무해한 사람        2.0                   0.0   \n",
       "4  9.790000e+12                     일론 머스크        7.0                   0.0   \n",
       "\n",
       "      revwCntt     revwNum  \n",
       "0            0   1316001.0  \n",
       "1        04-04    909679.0  \n",
       "2   1234567890  11616342.0  \n",
       "3  11111111111   1545516.0  \n",
       "4     1.11E+11  13229101.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using double backslashes\n",
    "df = pd.read_csv(\"k_review_final_utf8.csv\")\n",
    "\n",
    "# Using raw string literal\n",
    "# df = pd.read_csv(r\"C:\\Users\\fx567\\Downloads\\all_book_info.csv\")\n",
    "\n",
    "df.shape\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ffd297d-a985-437a-88d7-b2d9ec0e34cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 첫 번째 CSV 파일 경로\n",
    "file1_path = \"all_reviewRecommendCount.csv\"\n",
    "# 두 번째 CSV 파일 경로\n",
    "file2_path = 'final_plz/k_all_D_review_utf8.csv'\n",
    "# 첫 번째 CSV 파일 읽기\n",
    "# 'title'과 함께 필요한 컬럼만 선택하여 읽기\n",
    "df1 = pd.read_csv(file1_path, usecols=['revwNum', 'reviewRecommendCount'])\n",
    "# 두 번째 CSV 파일 읽기\n",
    "df2 = pd.read_csv(file2_path)\n",
    "# 두 데이터프레임을 공통 컬럼 'title'을 기준으로 병합 (inner 조인)\n",
    "merged_df = pd.merge(df1, df2, on='revwNum', how='inner')\n",
    "\n",
    "# 결과 출력\n",
    "merged_df.head(5)\n",
    "## 변경된 CSV 파일 저장하기\n",
    "output_file_path = 'final_plz/k_final_review_utf8.csv'\n",
    "####### ANSI로 인코딩\n",
    "########merged_df.to_csv(output_file_path, encoding='ANSI', index=False)\n",
    "#utf-8로 인코딩\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215f54a0-5b40-4ca5-995a-b722d278678a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71079\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad6d2e7f-eeaa-4ae5-bee9-f6026f229127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column order has been changed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일을 읽어옵니다.\n",
    "df = pd.read_csv('final_plz/k_final_review_utf8.csv')\n",
    "\n",
    "# 컬럼 순서를 변경합니다.\n",
    "new_column_order = ['ISBN', 'cmdtName', 'revwScore', 'reviewRecommendCount', 'revwCntt', 'revwNum']  # 새로운 컬럼 순서 리스트\n",
    "df = df[new_column_order]  # 컬럼 순서를 새로운 순서로 변경합니다.\n",
    "\n",
    "# 변경된 데이터프레임을 새로운 CSV 파일로 저장합니다.\n",
    "df.to_csv('k_review_utf8.csv',encoding='utf-8', index=False)  # index=False로 설정하여 인덱스를 CSV 파일에 포함하지 않습니다.\n",
    "\n",
    "print(\"Column order has been changed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b2226a-c468-4d64-bf80-7b8a02c6ec43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ssl\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#soup = BeautifulSoup(html_content, 'html.parser')\n",
    "data_id_list = []\n",
    "\n",
    "# saleCmdtid가 들어있는 CSV 파일 읽고 데이터 프레임으로\n",
    "csv_file_path = r\"2020best.csv\"  # CSV 파일 경로\n",
    "data_id_df = pd.read_csv(csv_file_path)  # CSV 파일을 DataFrame으로 읽기\n",
    "\n",
    "# 'data_id' 열의 값을 리스트로 가져오기\n",
    "data_id_list = data_id_df['SCID'].tolist()\n",
    "\n",
    "def bookRV(num, data_id):\n",
    "    ssl_ctx = ssl.create_default_context()\n",
    "    ssl_ctx.set_ciphers('DEFAULT@SECLEVEL=1')\n",
    "\n",
    "    # num=리뷰페이지 data_id위에서 만든 리스트에서 가져올 것\n",
    "    url = f\"https://product.kyobobook.co.kr/api/review/list?page={num}&pageLimit=10&reviewSort=001&revwPatrCode=000&saleCmdtid={data_id}\"\n",
    "    response = requests.get(url, verify=True, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        review_list = data['data']['reviewList']\n",
    "\n",
    "        # 필요한 정보를 추출하여 리스트로 정리 후 추출\n",
    "        formatted_reviews = []\n",
    "        for review in review_list:\n",
    "            formatted_review = {\n",
    "                'saleCmdtid': review['saleCmdtid'],\n",
    "                'cmdtName': review['cmdtName'],\n",
    "                'revwCntt': review['revwCntt'],\n",
    "                'revwRvgr': review['revwRvgr'],\n",
    "                'reviewRecommendCount': review['reviewRecommendCount']\n",
    "            }\n",
    "            formatted_reviews.append(formatted_review)\n",
    "        return formatted_reviews\n",
    "        \n",
    "    else:  # 실패시\n",
    "        print(f\"Failed to retrieve the page for page {num}.\")\n",
    "        return []\n",
    "\n",
    "import re\n",
    "\n",
    "for data_id in data_id_list:   # data_id순환\n",
    "    book_rvlist = []    # 리뷰 쌓을 리스트\n",
    "    cmdtName = None  # cmdtName 변수 초기화\n",
    "    for num in range(1, 10):   # 리뷰 페이지 순환\n",
    "        if not cmdtName:\n",
    "            cmdtName = bookRV(1, data_id)[0]['cmdtName']  # 첫 번째 페이지의 cmdtName 가져오기\n",
    "        book_rvlist.extend(bookRV(num, data_id))\n",
    "        \n",
    "    # 파일명에 있는 공백을 언더스코어로 대체하여 저장\n",
    "    cmdtName = re.sub(r'[^\\w\\s]', '', cmdtName).replace(\" \", \"_\")\n",
    "    df_review = pd.DataFrame(book_rvlist)\n",
    "    df_review.to_csv(f\"last2020\\\\{cmdtName}_{data_id}.csv\", encoding='utf-8-sig', index=False)\n",
    "    \n",
    "# 작업 끝나면 clear출력\n",
    "print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5439ecf-431c-4b99-86cb-173c06196a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d89585-baec-43c6-b3e3-0ab28fe172a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1167272-2385-4538-b999-701848c05386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def merge_csv_files(folder_path, output_folder):\n",
    "    # 폴더 안의 모든 csv 파일 가져오기\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    # 모든 CSV 파일에서 50개의 행씩 뽑아내어 병합\n",
    "    dfs = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
    "        dfs.append(df.head(50))  # 각 파일에서 첫 50개 행만 선택\n",
    "\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # UTF-8로 저장\n",
    "    merged_df.to_csv(os.path.join(output_folder, 'K_Book_Reveiw_f_23_utf8.csv'), index=False, encoding='utf-8')\n",
    "\n",
    "    # ANSI로 저장\n",
    "    #merged_df.to_csv(os.path.join(output_folder, 'K_Book_Reveiw_f_23_ansi.csv'), index=False, encoding='ANSI')\n",
    "\n",
    "# 폴더 경로 설정\n",
    "folder_path = r\"bookreview2023\"\n",
    "output_folder = \"k_review\"\n",
    "\n",
    "# 함수 호출\n",
    "merge_csv_files(folder_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac58e80-a6d8-4f89-a610-14e3f59170b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6642bef7-f82f-4cc8-8bfe-c0931bb4416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76850, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"k_review_utf8.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac20cafa-bd51-4992-a6ab-745fe78bda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 중복 삭제\n",
    "# CSV 파일을 읽어와 데이터프레임으로 변환\n",
    "df = pd.read_csv(\"k_review_utf8.csv\")\n",
    "df.shape\n",
    "# 중복된 행 중 첫 번째 행만 남기고 나머지는 삭제\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# 변경된 데이터프레임을 새로운 CSV 파일로 저장\n",
    "df.to_csv(\"k_review_finish_utf8.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94ec083-4ace-4095-b19f-d7c61ab37d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"k_review_finish_utf8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba82d27-0a48-4697-9637-4bf04c188e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56593, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652c864a-fb42-4edf-a8c9-5e66e9f94a90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Document is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 응답의 내용을 파싱합니다.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m tree \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mfromstring(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# XPath를 사용하여 원하는 요소를 선택합니다.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 예를 들어, 모든 링크의 텍스트를 추출하고 싶다면:\u001b[39;00m\n\u001b[0;32m     19\u001b[0m links \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mxpath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//*[@id=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReviewList1\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/div[1]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lxml\\html\\__init__.py:873\u001b[0m, in \u001b[0;36mfromstring\u001b[1;34m(html, base_url, parser, **kw)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m     is_full_html \u001b[38;5;241m=\u001b[39m _looks_like_full_html_unicode(html)\n\u001b[1;32m--> 873\u001b[0m doc \u001b[38;5;241m=\u001b[39m document_fromstring(html, parser\u001b[38;5;241m=\u001b[39mparser, base_url\u001b[38;5;241m=\u001b[39mbase_url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_full_html:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\lxml\\html\\__init__.py:761\u001b[0m, in \u001b[0;36mdocument_fromstring\u001b[1;34m(html, parser, ensure_head_body, **kw)\u001b[0m\n\u001b[0;32m    759\u001b[0m value \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39mfromstring(html, parser, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m etree\u001b[38;5;241m.\u001b[39mParserError(\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_head_body \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     value\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, Element(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mParserError\u001b[0m: Document is empty"
     ]
    }
   ],
   "source": [
    "# 총 리뷰 수 크롤링\n",
    "# //*[@id=\"ReviewList1\"]/div[1]/p\n",
    "\n",
    "\n",
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# 크롤링할 웹 페이지의 URL\n",
    "url = 'https://product.kyobobook.co.kr/detail/S000200746091'\n",
    "\n",
    "# 웹 페이지에 요청을 보내고 응답을 받습니다.\n",
    "response = requests.get(url)\n",
    "\n",
    "# 응답의 내용을 파싱합니다.\n",
    "tree = html.fromstring(response.content)\n",
    "\n",
    "# XPath를 사용하여 원하는 요소를 선택합니다.\n",
    "# 예를 들어, 모든 링크의 텍스트를 추출하고 싶다면:\n",
    "links = tree.xpath('//*[@id=\"ReviewList1\"]/div[1]')\n",
    "\n",
    "# 추출한 내용을 출력합니다.\n",
    "for link in links:\n",
    "    print(link)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
